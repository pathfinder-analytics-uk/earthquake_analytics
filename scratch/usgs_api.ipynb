{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d08b9eeb-5cb9-41b1-96ff-abf343b17df3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# USGS Earthquake API Deep Dive\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. How the USGS Earthquake API works\n",
    "2. The 20,000 record limit problem\n",
    "3. Single-threaded partitioning (the bottleneck)\n",
    "4. PySpark Custom Data Source (the scalable solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e863cd5-62fd-45ea-8996-5952a05b0148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Understanding the USGS API\n",
    "\n",
    "The USGS provides a free, public API for earthquake data. Let's explore how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b36cfe1-041c-4302-9cb0-9b2432472e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# The USGS Earthquake API endpoint\n",
    "USGS_API_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "# Let's fetch one week of earthquake data\n",
    "params = {\n",
    "    \"format\": \"geojson\",\n",
    "    \"starttime\": \"2000-01-01\",\n",
    "    \"endtime\": \"2000-01-07\"\n",
    "}\n",
    "\n",
    "response = requests.get(USGS_API_URL, params=params)\n",
    "data = response.json()\n",
    "\n",
    "print(f\"Response status: {response.status_code}\")\n",
    "print(f\"Total earthquake events: {len(data['features'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04f1dcc-8ac3-4b98-ab70-5fdbd269a3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploring the Response Structure\n",
    "\n",
    "The API returns GeoJSON format with:\n",
    "- `metadata`: Information about the query\n",
    "- `features`: Array of earthquake events\n",
    "- Each feature has `properties` (event details) and `geometry` (coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "058986cd-e1eb-4db1-abe2-7b8b2b77aa47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e2dcf3-a27f-4c27-8f99-a8d79ed13ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the metadata\n",
    "print(\"=== METADATA ===\")\n",
    "print(json.dumps(data['metadata'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd25b20-500e-47d7-ba1a-a783ef00dd52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now let's examine a single earthquake event\n",
    "print(\"=== SAMPLE EVENT ===\")\n",
    "event = data['features'][0]\n",
    "print(json.dumps(event, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9476e1ff-bcdc-497e-be91-54e7145da915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Properties We Care About\n",
    "\n",
    "Each earthquake event contains rich metadata. Let's convert the API response to a Spark DataFrame for easier viewing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45028921-5024-4c18-a549-fb3792569e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, DoubleType, LongType, IntegerType, MapType\n",
    ")\n",
    "\n",
    "# Defining the schema as the top level keys with nested arrays\n",
    "schema = StructType([\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"properties\", MapType(StringType(), StringType(), True), True),\n",
    "            StructField(\"geometry\", MapType(StringType(), StringType(), True), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "df = spark.createDataFrame(response.json()['features'], schema=schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5bbdfb7-c2f1-46a6-861d-9b28ea91793f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: The 20,000 Record Limit Problem\n",
    "\n",
    "The USGS API has a **hard limit of 20,000 records per request**.\n",
    "\n",
    "If your query would return more than 20,000 records, **the API returns a 400 error** - it doesn't silently truncate, it fails completely.\n",
    "\n",
    "Let's see what happens when we try to fetch a full year of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56e621d-7e3c-486e-8322-c7a34dfe6032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to fetch a full year - this will FAIL with a 400 error!\n",
    "params_year = {\n",
    "    \"format\": \"geojson\",\n",
    "    \"starttime\": \"2000-01-01\",\n",
    "    \"endtime\": \"2001-01-01\"\n",
    "}\n",
    "\n",
    "response_year = requests.get(USGS_API_URL, params=params_year)\n",
    "\n",
    "print(f\"Status Code: {response_year.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1a8c1b4-2c20-4134-beb3-8d48b7b4beef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Single-Threaded Partitioning (The Bottleneck)\n",
    "\n",
    "One solution is to break the date range into smaller chunks and make multiple requests.\n",
    "\n",
    "**Problem**: This runs sequentially - each request waits for the previous one to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be99146-38cd-4a92-866b-38a101e41ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (Demo) Fetching 12 Months of Data (Sequential)\n",
    "\n",
    "Watch how each chunk waits for the previous one to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232a3c5b-7f19-4cef-8c64-b6749c42cebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch 12 months of data using sequential approach\n",
    "# This will take a while because it's single-threaded!\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "start_s = \"2000-01-01\"\n",
    "end_s = \"2001-01-01\"\n",
    "num_partitions = 25\n",
    "\n",
    "\n",
    "# Explicit parsing to UTC\n",
    "start = datetime.strptime(start_s, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "end = datetime.strptime(end_s, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "\n",
    "total_seconds = (end - start).total_seconds()\n",
    "step = total_seconds / num_partitions\n",
    "\n",
    "parts = []\n",
    "for i in range(num_partitions):\n",
    "    p_start = start + timedelta(seconds=step * i)\n",
    "    p_end = start + timedelta(seconds=step * (i + 1))\n",
    "\n",
    "    # Explicit ISO-8601 UTC formatting (USGS-compatible)\n",
    "    start_iso = p_start.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_iso = p_end.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    parts.append({\"start_iso\":start_iso, \"end_iso\":end_iso, \"index\": i})\n",
    "\n",
    "import requests\n",
    "\n",
    "URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for part in parts:\n",
    "    print(f\"Partition {part.get('index')}: {len(all_features)} events\")\n",
    "\n",
    "    params = {\n",
    "        \"format\": \"geojson\",\n",
    "        \"starttime\": part.get(\"start_iso\"),\n",
    "        \"endtime\": part.get(\"end_iso\"),\n",
    "    }\n",
    "\n",
    "    response = requests.get(URL, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()                 # dict\n",
    "    features = data.get(\"features\", [])    # list\n",
    "\n",
    "    all_features.extend(features)           # ‚úÖ list.extend(list)\n",
    "\n",
    "print(\"Total events:\", len(all_features))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d87d050-68f6-42bf-8f19-99fef7e3f3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The Problem with Sequential Processing\n",
    "\n",
    "For 12 months of data with approx 14-day partitions:\n",
    "- ~25 partition to process\n",
    "- Each partition takes ~1-3 seconds (network latency + API processing)\n",
    "- Total time: **20-40 seconds** (or more!)\n",
    "\n",
    "For a longer time periods **it will take much longer**\n",
    "\n",
    "And remember - we HAVE to partition because the API **fails with a 400 error** if we exceed 20,000 records. A single month is safe, but anything larger requires partitioning.\n",
    "\n",
    "**This sequential approach doesn't scale!**\n",
    "\n",
    "What if we could process all chunks **in parallel**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55b5183f-7e01-4b92-84f6-4878339f23fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: PySpark Custom Data Source (The Solution)\n",
    "\n",
    "Spark's **Custom Data Source API** allows us to:\n",
    "1. Define how to partition the work (date ranges)\n",
    "2. Let Spark distribute partitions across executors\n",
    "3. Process all partitions **in parallel**\n",
    "\n",
    "Let's build it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b328d917-535d-4edf-a388-8f1e1cedec1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Define the InputPartition\n",
    "\n",
    "An `InputPartition` represents **one unit of work** that can be processed independently.\n",
    "\n",
    "In our case, each partition is a date range to fetch from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b12dac-a23e-40f7-8a9b-2d133f5ac055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceReader, InputPartition\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "\n",
    "class TimeRangePartition(InputPartition):\n",
    "    \"\"\"\n",
    "    Represents one partition of work - a specific date range to fetch.\n",
    "\n",
    "    This object gets serialized and sent to executors.\n",
    "    Keep it lightweight - just the data needed to fetch this chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, starttime: str, endtime: str, pid: int):\n",
    "        self.starttime = starttime  # ISO format string\n",
    "        self.endtime = endtime      # ISO format string\n",
    "        self.pid = pid              # Partition ID for logging/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64aa426d-f2ef-4474-9a0c-42d53c01cd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Define the DataSourceReader\n",
    "\n",
    "The `DataSourceReader` has two key methods:\n",
    "\n",
    "- **`partitions()`**: Runs on the **driver**. Creates the list of partitions.\n",
    "- **`read(partition)`**: Runs on **executors**. Fetches data for one partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26035338-d7b1-41ed-b8ca-faa39b6e20f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class USGSDataSourceReader(DataSourceReader):\n",
    "    \"\"\"\n",
    "    Reader that creates partitions and fetches data from USGS API.\n",
    "\n",
    "    - partitions() runs on the DRIVER (creates the work units)\n",
    "    - read() runs on EXECUTORS (does the actual API calls)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema, options):\n",
    "        self.schema = schema\n",
    "        self.options = options\n",
    "\n",
    "    def _parse_dt_utc(self, s: str) -> datetime:\n",
    "        \"\"\"Parse date string to UTC datetime.\"\"\"\n",
    "        # Handle simple 'YYYY-MM-DD' format\n",
    "        if len(s) == 10 and s[4] == \"-\" and s[7] == \"-\":\n",
    "            return datetime.strptime(s, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "\n",
    "        # Handle ISO8601 format\n",
    "        dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc)\n",
    "\n",
    "    def _to_usgs_iso(self, dt: datetime) -> str:\n",
    "        \"\"\"Convert datetime to USGS-compatible ISO string.\"\"\"\n",
    "        return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    def partitions(self):\n",
    "        \"\"\"\n",
    "        Create partitions based on the date range and numPartitions.\n",
    "\n",
    "        THIS RUNS ON THE DRIVER.\n",
    "\n",
    "        Returns a list of TimeRangePartition objects that Spark will\n",
    "        distribute across executors.\n",
    "        \"\"\"\n",
    "        # Get options from the user\n",
    "        start_s = self.options.get(\"starttime\")\n",
    "        end_s = self.options.get(\"endtime\")\n",
    "\n",
    "        if not start_s or not end_s:\n",
    "            raise ValueError(\n",
    "                \"You must provide .option('starttime', ...) and .option('endtime', ...)\"\n",
    "            )\n",
    "\n",
    "        num_partitions = int(self.options.get(\"numPartitions\", \"10\"))\n",
    "\n",
    "        if num_partitions <= 0:\n",
    "            raise ValueError(\"numPartitions must be > 0\")\n",
    "\n",
    "        # Parse dates\n",
    "        start = self._parse_dt_utc(start_s)\n",
    "        end = self._parse_dt_utc(end_s)\n",
    "\n",
    "        if end <= start:\n",
    "            raise ValueError(f\"endtime must be after starttime. Got {start_s} -> {end_s}\")\n",
    "\n",
    "        # Calculate time step per partition\n",
    "        total_seconds = (end - start).total_seconds()\n",
    "        step = total_seconds / num_partitions\n",
    "\n",
    "        # Create partition objects\n",
    "        parts = []\n",
    "        for i in range(num_partitions):\n",
    "            p_start = start + timedelta(seconds=step * i)\n",
    "            p_end = start + timedelta(seconds=step * (i + 1))\n",
    "\n",
    "            parts.append(\n",
    "                TimeRangePartition(\n",
    "                    starttime=self._to_usgs_iso(p_start),\n",
    "                    endtime=self._to_usgs_iso(p_end),\n",
    "                    pid=i\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(f\"‚úì Created {len(parts)} partitions for date range {start_s} to {end_s}\")\n",
    "        return parts\n",
    "\n",
    "    def read(self, partition: TimeRangePartition):\n",
    "        \"\"\"\n",
    "        Fetch data for a single partition from USGS API.\n",
    "\n",
    "        THIS RUNS ON EXECUTORS (potentially in parallel!).\n",
    "\n",
    "        Yields tuples that match the schema defined in the DataSource.\n",
    "        \"\"\"\n",
    "        # Import inside read() - this code runs on executors\n",
    "        # which may not have the same imports as the driver\n",
    "        import requests\n",
    "\n",
    "        url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "        params = {\n",
    "            \"format\": \"geojson\",\n",
    "            \"starttime\": partition.starttime,\n",
    "            \"endtime\": partition.endtime\n",
    "        }\n",
    "\n",
    "        # Make the API request\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "\n",
    "        # Handle errors with useful information\n",
    "        if r.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"USGS HTTP {r.status_code} partition={partition.pid} \"\n",
    "                f\"start={partition.starttime} end={partition.endtime}. \"\n",
    "                f\"Response: {r.text[:500]}\"\n",
    "            )\n",
    "\n",
    "        # Parse response and yield rows\n",
    "        payload = r.json()\n",
    "        features = payload.get(\"features\", [])\n",
    "\n",
    "        for f in features:\n",
    "            # Yield tuple matching our schema:\n",
    "            # (type, properties, geometry, id)\n",
    "            yield (\n",
    "                f.get(\"type\"),\n",
    "                f.get(\"properties\"),  # MapType handles dict automatically\n",
    "                f.get(\"geometry\"),    # MapType handles dict automatically\n",
    "                f.get(\"id\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0fc3e35-19d4-4bb1-9266-145bcc4e7b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Define the DataSource\n",
    "\n",
    "The `DataSource` is the entry point. It defines:\n",
    "- **`name()`**: The format string used in `spark.read.format(\"usgs\")`\n",
    "- **`schema()`**: The output schema of the DataFrame\n",
    "- **`reader()`**: Returns the DataSourceReader instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5adc2402-8bc8-4680-a456-83dd3f7efde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class USGSDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    Custom Spark DataSource for USGS Earthquake API.\n",
    "\n",
    "    Usage:\n",
    "        spark.dataSource.register(USGSDataSource)\n",
    "\n",
    "        df = spark.read.format(\"usgs\") \\\\\n",
    "            .option(\"starttime\", \"2000-01-01\") \\\\\n",
    "            .option(\"endtime\", \"2001-01-01\") \\\\\n",
    "            .option(\"numPartitions\", \"25\") \\\\\n",
    "            .load()\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        \"\"\"The format name used in spark.read.format()\"\"\"\n",
    "        return \"usgs\"\n",
    "\n",
    "    def schema(self):\n",
    "        \"\"\"\n",
    "        Define the output schema.\n",
    "\n",
    "        We keep properties and geometry as MapType to preserve\n",
    "        all the nested data from the API response.\n",
    "        \"\"\"\n",
    "        return StructType([\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"properties\", MapType(StringType(), StringType(), True), True),\n",
    "            StructField(\"geometry\", MapType(StringType(), StringType(), True), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "    def reader(self, schema: StructType):\n",
    "        \"\"\"Return a reader instance with the user's options.\"\"\"\n",
    "        return USGSDataSourceReader(schema, self.options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07573c0f-7922-438b-93bd-6045528d35ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Register and Use the DataSource\n",
    "\n",
    "Now let's register our custom DataSource with Spark and use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40461139-ed36-4398-b5a1-9f981bc43070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataSource\n",
    "spark.dataSource.register(USGSDataSource)\n",
    "print(\"‚úì USGS DataSource registered with Spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30440933-6ef7-451d-8aae-e6b0e8e33487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Fetching 12 Months of Data (Parallel!)\n",
    "\n",
    "Now let's fetch the same 12 months of data, but using our parallel DataSource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03ba8452-316b-4130-827f-cd5945d9b757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch the same 3 months of data - but now in PARALLEL!\n",
    "start_time = time.time()\n",
    "\n",
    "df = spark.read.format(\"usgs\") \\\n",
    "    .option(\"starttime\", \"2000-01-01\") \\\n",
    "    .option(\"endtime\", \"2001-01-01\") \\\n",
    "    .option(\"numPartitions\", \"25\") \\\n",
    "    .load()\n",
    "\n",
    "# Force execution by counting rows\n",
    "event_count = df.count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"üìä Total events fetched: {event_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3785115-5276-4ffa-8115-c7c5e3123d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compare the Results!\n",
    "\n",
    "| Approach | Time | Speedup |\n",
    "|----------|------|---------|\n",
    "| Sequential (single-threaded) | ~30-40 seconds | 1x |\n",
    "| PySpark DataSource (parallel) | ~5-10 seconds | **4-6x faster!** |\n",
    "\n",
    "The speedup comes from:\n",
    "1. **Parallel API calls** - Multiple executors fetch data simultaneously\n",
    "2. **Distributed processing** - Work is spread across the cluster\n",
    "3. **No waiting** - Partitions don't wait for each other\n",
    "\n",
    "And remember - partitioning isn't optional! The API returns a **400 error** if we exceed 20,000 records. The question is whether we partition sequentially (slow) or in parallel (fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dedc4be-94c6-4a54-b74d-c8923c56a55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the data we fetched\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea3e6bc-c8ea-45f7-9b1a-5fa6345997c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Understanding the Schema\n",
    "\n",
    "The `properties` and `geometry` columns are `MapType` - they contain all the nested data from the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b4adba0-4110-426c-9ab4-1b53bc40a6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract specific properties from the map\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_expanded = df.select(\n",
    "    col(\"id\"),\n",
    "    col(\"properties\")[\"mag\"].alias(\"magnitude\"),\n",
    "    col(\"properties\")[\"place\"].alias(\"place\"),\n",
    "    col(\"properties\")[\"sig\"].alias(\"significance\"),\n",
    "    col(\"properties\")[\"time\"].alias(\"event_time\"),\n",
    "    col(\"geometry\")[\"coordinates\"].alias(\"coordinates\")\n",
    ")\n",
    "\n",
    "display(df_expanded.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ace0766-aa5b-4e81-b88d-50b579e9c177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Purpose | Runs On |\n",
    "|-----------|---------|---------|\n",
    "| `InputPartition` | Defines one unit of work (date range) | Serialized to executors |\n",
    "| `DataSourceReader.partitions()` | Creates all partition objects | Driver |\n",
    "| `DataSourceReader.read()` | Fetches data for one partition | Executors (parallel!) |\n",
    "| `DataSource` | Entry point, defines schema and name | Driver |\n",
    "\n",
    "**Key Insight**: The magic happens because `read()` runs on executors. Spark automatically:\n",
    "1. Distributes partitions across available executors\n",
    "2. Calls `read()` in parallel on each executor\n",
    "3. Combines the results into a single DataFrame\n",
    "\n",
    "This pattern works for ANY API or data source - not just USGS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "067380d4-2b97-4a45-b3b9-333370139c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In the actual ETL pipeline (`etl/bronze_ingestion.py`), we use this exact DataSource to:\n",
    "1. Ingest earthquake data into the Bronze layer\n",
    "2. Store raw data in Delta format with Change Data Feed enabled\n",
    "3. Track ingestion progress with checkpoints\n",
    "\n",
    "The implementation in `utils/datasource.py` is identical to what we built here!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "usgs_api",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
