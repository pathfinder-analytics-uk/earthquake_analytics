{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Gold (Analytics) Layer Aggregation\n",
    "\n",
    "Create business-ready aggregated views for the Streamlit dashboard.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Pre-aggregated tables for fast queries\n",
    "- Multiple grain levels (event, daily, regional)\n",
    "- Optimized for visualization\n",
    "\n",
    "**Source:** `{catalog}.{schema}.silver_events`  \n",
    "**Targets:**\n",
    "- `gold_events_map` - Individual events for map display\n",
    "- `gold_daily_summary` - Daily aggregations\n",
    "- `gold_regional_summary` - Regional aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dbutils.widgets.text(\"catalog\", \"earthquakes_dev\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"usgs\", \"Schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, max as spark_max, min as spark_min, sum as spark_sum,\n",
    "    round as spark_round, current_timestamp, date_trunc, when, to_date\n",
    ")\n",
    "\n",
    "from utils.helpers import (\n",
    "    get_table_path, \n",
    "    write_delta_table, \n",
    "    write_delta_table_with_cdf,\n",
    "    print_table_stats,\n",
    "    read_incremental_or_full,\n",
    "    save_checkpoint,\n",
    "    table_exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table paths\n",
    "source_table = get_table_path(catalog, schema, \"silver_events\")\n",
    "checkpoint_table = get_table_path(catalog, schema, \"_checkpoints\")\n",
    "\n",
    "print(f\"Source: {source_table}\")\n",
    "print(f\"Checkpoint: {checkpoint_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Silver Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read incrementally using CDF (only new/changed records since last run)\n",
    "df_changes, source_version, is_incremental = read_incremental_or_full(\n",
    "    spark, source_table, checkpoint_table\n",
    ")\n",
    "\n",
    "changes_count = df_changes.count()\n",
    "print(f\"Changed records to process: {changes_count:,}\")\n",
    "\n",
    "if changes_count == 0:\n",
    "    print(\"No new records to process, exiting\")\n",
    "    dbutils.notebook.exit(\"success\")\n",
    "\n",
    "# Drop CDF metadata columns if present\n",
    "cdf_cols = [\"_change_type\", \"_commit_version\", \"_commit_timestamp\"]\n",
    "for c in cdf_cols:\n",
    "    if c in df_changes.columns:\n",
    "        df_changes = df_changes.drop(c)\n",
    "\n",
    "# For incremental processing, we need to identify affected dates and regions\n",
    "# so we can recompute only those aggregations\n",
    "if is_incremental:\n",
    "    affected_dates = [row[\"date\"] for row in df_changes.select(\n",
    "        to_date(col(\"event_time\")).alias(\"date\")\n",
    "    ).distinct().collect()]\n",
    "    \n",
    "    affected_regions = [row[\"region\"] for row in df_changes.select(\n",
    "        col(\"region\")\n",
    "    ).filter(col(\"region\").isNotNull()).distinct().collect()]\n",
    "    \n",
    "    print(f\"Affected dates: {len(affected_dates)}\")\n",
    "    print(f\"Affected regions: {len(affected_regions)}\")\n",
    "else:\n",
    "    affected_dates = None\n",
    "    affected_regions = None\n",
    "\n",
    "# For gold layer aggregations, we need the full silver data for affected partitions\n",
    "df = spark.table(source_table)\n",
    "print(f\"Total silver records: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Table 1: Events for Map Display\n",
    "\n",
    "Denormalized view with only columns needed for the map visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity category based on SIGNIFICANCE (not magnitude)\n",
    "# This is more consistent as significance accounts for multiple factors\n",
    "df_map = df.select(\n",
    "    # Identifiers\n",
    "    col(\"event_id\"),\n",
    "    \n",
    "    # Location (required for map)\n",
    "    col(\"latitude\"),\n",
    "    col(\"longitude\"),\n",
    "    col(\"place\"),\n",
    "    col(\"region\"),\n",
    "    \n",
    "    # Magnitude info (kept for reference)\n",
    "    col(\"magnitude\"),\n",
    "    col(\"magnitude_category\"),\n",
    "    \n",
    "    # Significance (primary metric for sizing/coloring)\n",
    "    col(\"significance\"),\n",
    "    (col(\"significance\") / 100.0).alias(\"size_factor\"),\n",
    "    \n",
    "    # Depth info\n",
    "    col(\"depth_km\"),\n",
    "    col(\"depth_category\"),\n",
    "    \n",
    "    # Timing\n",
    "    col(\"event_time\"),\n",
    "    \n",
    "    # Alert/Impact\n",
    "    col(\"alert_level\"),\n",
    "    col(\"has_tsunami_warning\"),\n",
    "    col(\"felt_reports\"),\n",
    "    \n",
    "    # Severity category based on SIGNIFICANCE score\n",
    "    # Thresholds: severe >= 600, major >= 400, moderate >= 200, minor >= 100, low < 100\n",
    "    when(col(\"significance\") >= 600, \"severe\")\n",
    "    .when(col(\"significance\") >= 400, \"major\")\n",
    "    .when(col(\"significance\") >= 200, \"moderate\")\n",
    "    .when(col(\"significance\") >= 100, \"minor\")\n",
    "    .otherwise(\"low\").alias(\"severity\"),\n",
    "    \n",
    "    # URL for details\n",
    "    col(\"detail_url\"),\n",
    "    \n",
    "    # Processing timestamp\n",
    "    current_timestamp().alias(\"_processed_at\")\n",
    ").filter(\n",
    "    # Only events with valid coordinates\n",
    "    col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"Map events: {df_map.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write gold_events_map\n",
    "# For map events, we can use merge with event_id as key for incremental updates\n",
    "table_path = get_table_path(catalog, schema, \"gold_events_map\")\n",
    "write_delta_table_with_cdf(\n",
    "    df_map, \n",
    "    table_path, \n",
    "    mode=\"merge\",\n",
    "    merge_keys=[\"event_id\"],\n",
    "    enable_cdf=False  # Gold layer doesn't need CDF (end of pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Table 2: Daily Summary\n",
    "\n",
    "Aggregated statistics by day for time-series charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For incremental processing, only aggregate affected dates\n",
    "if is_incremental and affected_dates:\n",
    "    df_to_agg = df.filter(to_date(col(\"event_time\")).isin(affected_dates))\n",
    "    print(f\"Aggregating {df_to_agg.count():,} records for {len(affected_dates)} affected dates\")\n",
    "else:\n",
    "    df_to_agg = df\n",
    "    print(f\"Full aggregation of {df_to_agg.count():,} records\")\n",
    "\n",
    "df_daily = df_to_agg.groupBy(\n",
    "    date_trunc(\"day\", col(\"event_time\")).alias(\"date\")\n",
    ").agg(\n",
    "    # Counts\n",
    "    count(\"*\").alias(\"total_events\"),\n",
    "    count(when(col(\"significance\") >= 500, 1)).alias(\"high_significance_events\"),\n",
    "    count(when(col(\"has_tsunami_warning\"), 1)).alias(\"tsunami_warnings\"),\n",
    "    \n",
    "    # Significance stats (primary metric)\n",
    "    spark_round(avg(\"significance\"), 0).alias(\"avg_significance\"),\n",
    "    spark_max(\"significance\").alias(\"max_significance\"),\n",
    "    spark_min(\"significance\").alias(\"min_significance\"),\n",
    "    \n",
    "    # Magnitude stats (secondary, for reference)\n",
    "    spark_round(avg(\"magnitude\"), 2).alias(\"avg_magnitude\"),\n",
    "    spark_max(\"magnitude\").alias(\"max_magnitude\"),\n",
    "    \n",
    "    # Depth stats\n",
    "    spark_round(avg(\"depth_km\"), 2).alias(\"avg_depth_km\"),\n",
    "    \n",
    "    # Impact\n",
    "    spark_sum(\"felt_reports\").alias(\"total_felt_reports\"),\n",
    "    \n",
    "    # Counts by significance category\n",
    "    count(when(col(\"significance\") >= 600, 1)).alias(\"count_severe\"),\n",
    "    count(when((col(\"significance\") >= 400) & (col(\"significance\") < 600), 1)).alias(\"count_major\"),\n",
    "    count(when((col(\"significance\") >= 200) & (col(\"significance\") < 400), 1)).alias(\"count_moderate\"),\n",
    "    count(when((col(\"significance\") >= 100) & (col(\"significance\") < 200), 1)).alias(\"count_minor\"),\n",
    "    count(when(col(\"significance\") < 100, 1)).alias(\"count_low\"),\n",
    ").withColumn(\"_processed_at\", current_timestamp())\n",
    "\n",
    "print(f\"Daily records: {df_daily.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write gold_daily_summary - use merge to update only affected dates\n",
    "table_path = get_table_path(catalog, schema, \"gold_daily_summary\")\n",
    "write_delta_table(\n",
    "    df_daily, \n",
    "    table_path, \n",
    "    mode=\"merge\",\n",
    "    merge_keys=[\"date\"]  # Merge on date to update existing aggregations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Table 3: Regional Summary\n",
    "\n",
    "Aggregated statistics by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For incremental processing, only aggregate affected regions\n",
    "if is_incremental and affected_regions:\n",
    "    df_to_agg = df.filter(col(\"region\").isin(affected_regions))\n",
    "    print(f\"Aggregating {df_to_agg.count():,} records for {len(affected_regions)} affected regions\")\n",
    "else:\n",
    "    df_to_agg = df\n",
    "    print(f\"Full aggregation of {df_to_agg.count():,} records\")\n",
    "\n",
    "df_regional = df_to_agg.filter(col(\"region\").isNotNull()).groupBy(\n",
    "    col(\"region\")\n",
    ").agg(\n",
    "    # Counts\n",
    "    count(\"*\").alias(\"total_events\"),\n",
    "    count(when(col(\"significance\") >= 500, 1)).alias(\"high_significance_events\"),\n",
    "    \n",
    "    # Significance stats (primary metric)\n",
    "    spark_round(avg(\"significance\"), 0).alias(\"avg_significance\"),\n",
    "    spark_max(\"significance\").alias(\"max_significance\"),\n",
    "    \n",
    "    # Magnitude stats (secondary, for reference)\n",
    "    spark_round(avg(\"magnitude\"), 2).alias(\"avg_magnitude\"),\n",
    "    spark_max(\"magnitude\").alias(\"max_magnitude\"),\n",
    "    \n",
    "    # Depth stats\n",
    "    spark_round(avg(\"depth_km\"), 2).alias(\"avg_depth_km\"),\n",
    "    \n",
    "    # Time range\n",
    "    spark_min(\"event_time\").alias(\"first_event\"),\n",
    "    spark_max(\"event_time\").alias(\"last_event\"),\n",
    "    \n",
    "    # Centroid for map\n",
    "    spark_round(avg(\"latitude\"), 4).alias(\"centroid_lat\"),\n",
    "    spark_round(avg(\"longitude\"), 4).alias(\"centroid_lon\"),\n",
    ").withColumn(\"_processed_at\", current_timestamp())\n",
    "\n",
    "print(f\"Regional records: {df_regional.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write gold_regional_summary - use merge to update only affected regions\n",
    "table_path = get_table_path(catalog, schema, \"gold_regional_summary\")\n",
    "write_delta_table(\n",
    "    df_regional, \n",
    "    table_path, \n",
    "    mode=\"merge\",\n",
    "    merge_keys=[\"region\"]  # Merge on region to update existing aggregations\n",
    ")\n",
    "\n",
    "# Save checkpoint for silver -> gold processing\n",
    "save_checkpoint(spark, checkpoint_table, source_table, source_version, changes_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of all gold tables\n",
    "gold_tables = [\"gold_events_map\", \"gold_daily_summary\", \"gold_regional_summary\"]\n",
    "\n",
    "for table in gold_tables:\n",
    "    table_path = get_table_path(catalog, schema, table)\n",
    "    print_table_stats(spark, table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from each table\n",
    "print(\"\\n=== gold_events_map sample ===\")\n",
    "spark.table(get_table_path(catalog, schema, \"gold_events_map\")) \\\n",
    "    .select(\"event_id\", \"significance\", \"severity\", \"region\", \"size_factor\") \\\n",
    "    .show(5)\n",
    "\n",
    "print(\"\\n=== gold_daily_summary sample ===\")\n",
    "spark.table(get_table_path(catalog, schema, \"gold_daily_summary\")) \\\n",
    "    .orderBy(col(\"date\").desc()) \\\n",
    "    .select(\"date\", \"total_events\", \"avg_significance\", \"max_significance\") \\\n",
    "    .show(5)\n",
    "\n",
    "print(\"\\n=== gold_regional_summary sample ===\")\n",
    "spark.table(get_table_path(catalog, schema, \"gold_regional_summary\")) \\\n",
    "    .orderBy(col(\"total_events\").desc()) \\\n",
    "    .select(\"region\", \"total_events\", \"avg_significance\", \"max_significance\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return success\n",
    "dbutils.notebook.exit(\"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
