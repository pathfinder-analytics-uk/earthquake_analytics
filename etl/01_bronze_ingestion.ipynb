{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Raw (Bronze) Layer Ingestion\n",
    "\n",
    "Ingest earthquake data from USGS API into the bronze layer.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Custom PySpark DataSource for USGS API\n",
    "- Idempotent writes using Delta Lake merge\n",
    "- Parameterized notebooks with widgets\n",
    "\n",
    "**Table:** `{catalog}.{schema}.raw_events`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set default dates\n",
    "default_start_date=(datetime.now()- timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "default_end_date=(datetime.now()- timedelta(days=1)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - these become Databricks widgets\n",
    "dbutils.widgets.text(\"catalog\", \"earthquakes_dev\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"usgs\", \"Schema\")\n",
    "dbutils.widgets.text(\"start_date\", \"\", \"Start Date (YYYY-MM-DD)\")\n",
    "dbutils.widgets.text(\"end_date\", \"\", \"End Date (YYYY-MM-DD)\")\n",
    "dbutils.widgets.text(\"num_partitions\", \"8\", \"Number of Partitions\")\n",
    "dbutils.widgets.dropdown(\"write_mode\", \"merge\", [\"merge\", \"overwrite\", \"append\"], \"Write Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "start_date = dbutils.widgets.get(\"start_date\") if dbutils.widgets.get(\"start_date\")!=\"\" else default_start_date\n",
    "end_date = dbutils.widgets.get(\"end_date\") if dbutils.widgets.get(\"end_date\")!=\"\" else default_end_date\n",
    "num_partitions = dbutils.widgets.get(\"num_partitions\")\n",
    "write_mode = dbutils.widgets.get(\"write_mode\")\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"End Date: {end_date}\")\n",
    "print(f\"Write Mode: {write_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities from the installed wheel\n",
    "from utils.helpers import (\n",
    "            get_or_create_catalog_schema,\n",
    "            get_table_path,\n",
    "            write_delta_table_with_cdf,\n",
    "            add_metadata_columns,\n",
    "            print_table_stats,\n",
    "        )\n",
    "\n",
    "from utils.datasource import register_usgs_datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure catalog and schema exist\n",
    "get_or_create_catalog_schema(spark, catalog, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the USGS DataSource\n",
    "register_usgs_datasource(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from USGS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestion date range\n",
    "print(f\"Ingesting data from {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from USGS API using custom DataSource\n",
    "df_raw = spark.read.format(\"usgs\") \\\n",
    "    .option(\"starttime\", start_date) \\\n",
    "    .option(\"endtime\", end_date) \\\n",
    "    .option(\"numPartitions\", num_partitions) \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Fetched {df_raw.count():,} events from USGS API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df_raw.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema shows MapType for properties and geometry\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata columns\n",
    "df_raw = add_metadata_columns(df_raw)\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table path\n",
    "table_path = get_table_path(catalog, schema, \"bronze_events\")\n",
    "print(f\"Target table: {table_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta table (idempotent with merge, CDF enabled for downstream incremental processing)\n",
    "record_count = write_delta_table_with_cdf(\n",
    "    df=df_raw,\n",
    "    table_path=table_path,\n",
    "    mode=write_mode,\n",
    "    merge_keys=[\"id\"],  # Event ID is unique\n",
    "    enable_cdf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table statistics\n",
    "print_table_stats(spark, table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation query\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_events,\n",
    "        MIN(_ingested_at) as earliest_ingestion,\n",
    "        MAX(_ingested_at) as latest_ingestion\n",
    "    FROM {table_path}\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return record count for job orchestration\n",
    "dbutils.notebook.exit(str(record_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
