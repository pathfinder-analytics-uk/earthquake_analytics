{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Silver (Enriched) Layer Transformation\n",
    "\n",
    "Transform raw earthquake data into typed, enriched format.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Schema enforcement and type casting\n",
    "- Extracting from MapType columns\n",
    "- Data quality and deduplication\n",
    "- Incremental processing\n",
    "\n",
    "**Source:** `{catalog}.{schema}.bronze_events`  \n",
    "**Target:** `{catalog}.{schema}.silver_events`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dbutils.widgets.text(\"catalog\", \"earthquakes_dev\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"usgs\", \"Schema\")\n",
    "dbutils.widgets.dropdown(\"write_mode\", \"merge\", [\"merge\", \"overwrite\"], \"Write Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "write_mode = dbutils.widgets.get(\"write_mode\")\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")\n",
    "print(f\"Write Mode: {write_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, from_unixtime, current_timestamp,\n",
    "    when, regexp_extract, trim, element_at\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType, IntegerType, TimestampType, LongType\n",
    ")\n",
    "\n",
    "from utils.helpers import (\n",
    "    get_table_path,\n",
    "    write_delta_table_with_cdf,\n",
    "    print_table_stats,\n",
    "    table_exists,\n",
    "    read_incremental_or_full,\n",
    "    save_checkpoint,\n",
    "    get_current_table_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table paths\n",
    "source_table = get_table_path(catalog, schema, \"bronze_events\")\n",
    "target_table = get_table_path(catalog, schema, \"silver_events\")\n",
    "checkpoint_table = get_table_path(catalog, schema, \"_checkpoints\")\n",
    "\n",
    "print(f\"Source: {source_table}\")\n",
    "print(f\"Target: {target_table}\")\n",
    "print(f\"Checkpoint: {checkpoint_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Bronze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read incrementally using CDF (only new/changed records since last run)\n",
    "# Falls back to full read on first run or if CDF not available\n",
    "df, source_version, is_incremental = read_incremental_or_full(\n",
    "    spark, source_table, checkpoint_table\n",
    ")\n",
    "\n",
    "record_count = df.count()\n",
    "print(f\"Records to process: {record_count:,}\")\n",
    "\n",
    "if record_count == 0:\n",
    "    print(\"No new records to process, exiting\")\n",
    "    dbutils.notebook.exit(\"0\")\n",
    "\n",
    "# Drop CDF metadata columns if present (from incremental read)\n",
    "cdf_cols = [\"_change_type\", \"_commit_version\", \"_commit_timestamp\"]\n",
    "for c in cdf_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(c)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform: Extract Fields from MapType Columns\n",
    "\n",
    "The bronze layer stores `properties` and `geometry` as MapType columns. We extract specific fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all fields from properties and geometry MapType columns\n",
    "# Using element_at() or direct map access with [\"key\"]\n",
    "df_typed = df.select(\n",
    "    # Primary key\n",
    "    col(\"id\").alias(\"event_id\"),\n",
    "    \n",
    "    # Magnitude info - extract from properties map\n",
    "    col(\"properties\")[\"mag\"].cast(DoubleType()).alias(\"magnitude\"),\n",
    "    col(\"properties\")[\"magType\"].alias(\"magnitude_type\"),\n",
    "    \n",
    "    # Location info\n",
    "    col(\"properties\")[\"place\"].alias(\"place\"),\n",
    "    \n",
    "    # Timing - USGS uses milliseconds since epoch\n",
    "    from_unixtime(\n",
    "        col(\"properties\")[\"time\"].cast(LongType()) / 1000\n",
    "    ).cast(TimestampType()).alias(\"event_time\"),\n",
    "    from_unixtime(\n",
    "        col(\"properties\")[\"updated\"].cast(LongType()) / 1000\n",
    "    ).cast(TimestampType()).alias(\"updated_time\"),\n",
    "    col(\"properties\")[\"tz\"].cast(IntegerType()).alias(\"timezone_offset\"),\n",
    "    \n",
    "    # Quality metrics\n",
    "    col(\"properties\")[\"rms\"].cast(DoubleType()).alias(\"rms\"),\n",
    "    col(\"properties\")[\"gap\"].cast(DoubleType()).alias(\"gap\"),\n",
    "    col(\"properties\")[\"dmin\"].cast(DoubleType()).alias(\"dmin\"),\n",
    "    col(\"properties\")[\"nst\"].cast(IntegerType()).alias(\"station_count\"),\n",
    "    col(\"properties\")[\"sig\"].cast(IntegerType()).alias(\"significance\"),\n",
    "    \n",
    "    # Impact metrics\n",
    "    col(\"properties\")[\"felt\"].cast(IntegerType()).alias(\"felt_reports\"),\n",
    "    col(\"properties\")[\"cdi\"].cast(DoubleType()).alias(\"cdi\"),\n",
    "    col(\"properties\")[\"mmi\"].cast(DoubleType()).alias(\"mmi\"),\n",
    "    col(\"properties\")[\"alert\"].alias(\"alert_level\"),\n",
    "    col(\"properties\")[\"tsunami\"].cast(IntegerType()).alias(\"tsunami_flag\"),\n",
    "    \n",
    "    # Source info\n",
    "    col(\"properties\")[\"net\"].alias(\"network\"),\n",
    "    col(\"properties\")[\"code\"].alias(\"event_code\"),\n",
    "    col(\"properties\")[\"status\"].alias(\"review_status\"),\n",
    "    \n",
    "    # URLs\n",
    "    col(\"properties\")[\"url\"].alias(\"detail_url\"),\n",
    "    col(\"properties\")[\"detail\"].alias(\"api_detail_url\"),\n",
    "    \n",
    "    # Coordinates from geometry map - coordinates is a string like \"[-122.5, 37.8, 10.0]\"\n",
    "    # We need to parse the coordinates string\n",
    "    col(\"geometry\")[\"coordinates\"].alias(\"coordinates_str\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse coordinates string [lon, lat, depth] \n",
    "# The coordinates come as a string representation of array\n",
    "from pyspark.sql.functions import regexp_extract, split, expr\n",
    "\n",
    "# Extract coordinates using regex or split\n",
    "# Format: \"[-122.5, 37.8, 10.0]\" or similar\n",
    "df_coords = df_typed.withColumn(\n",
    "    \"coords_clean\",\n",
    "    regexp_extract(col(\"coordinates_str\"), r\"\\[([^\\]]+)\\]\", 1)\n",
    ").withColumn(\n",
    "    \"coords_array\",\n",
    "    split(col(\"coords_clean\"), \",\")\n",
    ").withColumn(\n",
    "    \"longitude\",\n",
    "    trim(col(\"coords_array\")[0]).cast(DoubleType())\n",
    ").withColumn(\n",
    "    \"latitude\", \n",
    "    trim(col(\"coords_array\")[1]).cast(DoubleType())\n",
    ").withColumn(\n",
    "    \"depth_km\",\n",
    "    trim(col(\"coords_array\")[2]).cast(DoubleType())\n",
    ").drop(\"coordinates_str\", \"coords_clean\", \"coords_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coords.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich: Add Derived Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add magnitude category\n",
    "df_enriched = df_coords.withColumn(\n",
    "    \"magnitude_category\",\n",
    "    when(col(\"magnitude\") >= 8.0, \"great\")\n",
    "    .when(col(\"magnitude\") >= 7.0, \"major\")\n",
    "    .when(col(\"magnitude\") >= 6.0, \"strong\")\n",
    "    .when(col(\"magnitude\") >= 5.0, \"moderate\")\n",
    "    .when(col(\"magnitude\") >= 4.0, \"light\")\n",
    "    .when(col(\"magnitude\") >= 2.0, \"minor\")\n",
    "    .otherwise(\"micro\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add depth category\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"depth_category\",\n",
    "    when(col(\"depth_km\") < 70, \"shallow\")\n",
    "    .when(col(\"depth_km\") < 300, \"intermediate\")\n",
    "    .otherwise(\"deep\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract region from place string\n",
    "# Place format: \"10km NW of San Francisco, California\" -> \"San Francisco, California\"\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"region\",\n",
    "    when(\n",
    "        col(\"place\").contains(\" of \"),\n",
    "        trim(regexp_extract(col(\"place\"), r\" of (.+)$\", 1))\n",
    "    ).otherwise(col(\"place\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add alert level numeric (for sorting/filtering)\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"alert_level_numeric\",\n",
    "    when(col(\"alert_level\") == \"red\", 4)\n",
    "    .when(col(\"alert_level\") == \"orange\", 3)\n",
    "    .when(col(\"alert_level\") == \"yellow\", 2)\n",
    "    .when(col(\"alert_level\") == \"green\", 1)\n",
    "    .otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add boolean flags\n",
    "df_enriched = df_enriched \\\n",
    "    .withColumn(\"has_tsunami_warning\", col(\"tsunami_flag\") == 1) \\\n",
    "    .withColumn(\"is_reviewed\", col(\"review_status\") == \"reviewed\") \\\n",
    "    .withColumn(\"is_significant\", col(\"significance\") >= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add processing timestamp\n",
    "df_enriched = df_enriched.withColumn(\"_processed_at\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality: Filter and Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out records with missing critical fields\n",
    "df_clean = df_enriched.filter(\n",
    "    col(\"event_id\").isNotNull() &\n",
    "    col(\"magnitude\").isNotNull() &\n",
    "    col(\"latitude\").isNotNull() &\n",
    "    col(\"longitude\").isNotNull() &\n",
    "    col(\"event_time\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"Records after filtering: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate by event_id, keeping the most recently updated\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "window = Window.partitionBy(\"event_id\").orderBy(desc(\"updated_time\"))\n",
    "\n",
    "df_deduped = df_clean \\\n",
    "    .withColumn(\"_row_num\", row_number().over(window)) \\\n",
    "    .filter(col(\"_row_num\") == 1) \\\n",
    "    .drop(\"_row_num\")\n",
    "\n",
    "print(f\"Records after deduplication: {df_deduped.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview enriched data\n",
    "df_deduped.select(\n",
    "    \"event_id\", \"magnitude\", \"magnitude_category\", \n",
    "    \"place\", \"region\", \"latitude\", \"longitude\",\n",
    "    \"depth_km\", \"depth_category\", \"event_time\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Silver Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to silver table (with CDF enabled for downstream gold processing)\n",
    "record_count = write_delta_table_with_cdf(\n",
    "    df=df_deduped,\n",
    "    table_path=target_table,\n",
    "    mode=write_mode,\n",
    "    merge_keys=[\"event_id\"],\n",
    "    enable_cdf=True\n",
    ")\n",
    "\n",
    "# Save checkpoint so next run only processes new data\n",
    "save_checkpoint(spark, checkpoint_table, source_table, source_version, record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table statistics\n",
    "print_table_stats(spark, target_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_events,\n",
    "        COUNT(DISTINCT region) as unique_regions,\n",
    "        ROUND(AVG(magnitude), 2) as avg_magnitude,\n",
    "        MAX(magnitude) as max_magnitude,\n",
    "        SUM(CASE WHEN is_significant THEN 1 ELSE 0 END) as significant_events,\n",
    "        SUM(CASE WHEN has_tsunami_warning THEN 1 ELSE 0 END) as tsunami_warnings\n",
    "    FROM {target_table}\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by magnitude category\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        magnitude_category,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(significance), 0) as avg_significance\n",
    "    FROM {target_table}\n",
    "    GROUP BY magnitude_category\n",
    "    ORDER BY avg_significance DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return record count\n",
    "dbutils.notebook.exit(str(record_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
